{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXI44TXVgdMJ"
      },
      "source": [
        "# Final Project: Applied Mathematical Concepts For Deep Learning\n",
        "\n",
        "# Food Reviews Sentiment Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytvhQJhX3QmD"
      },
      "source": [
        "# **About The Project:**\n",
        "\n",
        "In this project, we are solving a binary classification problem for food reviews. We analysed the text written by customers in natural language, cleaned the data, trained models with multiple approaches and performing prediction with sample testing data.\n",
        "\n",
        "**About Dataset:**\n",
        "\n",
        "This dataset is taken from Kaggle and it consists of 500,000 reviews of amazon fine foods collected over 10 years of time and with the file size of 300 MB. We have used the 'Score' which is the rating of the the product out of 5 and the text reviews written by the customers.\n",
        "\n",
        "We used this data to perform a binary classification to determine whether the review submitted by the customer is positive or negative.\n",
        "\n",
        "We performed different experiments with various type of neural networks and finally selected 2 approaches to demonstrate.\n",
        "\n",
        "**Approach-1:** Using Bi-Directional LSTM to train the model from scratch.\n",
        "\n",
        "**Approach-2:** Fine-Tuning DistilBERT model with customized data.\n",
        "\n",
        "This notebook contains the codes and comments as per google standards of documenting and follows the object-oriented style of programming. Each cell contains a pipeline and at last we are calling all the functions in different class to run the training and prediction codes.\n",
        "\n",
        "\n",
        "Data Source: [Kaggle Link](https://www.kaggle.com/code/sonalisingh1411/nlp-part-1-amazon-fine-food-sentiment-analysis/input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DzH7wdY96wh"
      },
      "source": [
        "# Data Loading Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrBLTcIN9e-N",
        "outputId": "4c7535eb-b02d-41d0-f41a-db42fbf5d1cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" Load the google drive \"\"\"\n",
        "from google.colab import drive\n",
        "\n",
        "\"\"\" Data manipulation libraries \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\" NLTK Libraries and modules \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\"\"\" Other libraries \"\"\"\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "\"\"\" Scikit-learn libraries \"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\"\"\" Tensorflow libraries \"\"\"\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\"\"\" Import libraries for pre-trained model \"\"\"\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "\"\"\" Download nltk \"\"\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n79SqBh93L4"
      },
      "source": [
        "# Pre-Processing Pipeline\n",
        "We made the pre-processing pipeline to be used for training, testing and inference data. Here are the main operations we performed on the text data.\n",
        "\n",
        "**Data Cleaning:**\n",
        "1.   Remove punctuations\n",
        "2.   Convert words into lower case\n",
        "3.   Remove stop words\n",
        "4.   Stem the words\n",
        "5.   Remove the non-alphanumeric words\n",
        "\n",
        "We took 'Score' column for the output which is the rating out of 5. So, we labeled the output to give either 0 or 1.\n",
        "\n",
        "**Label Encodings:**\n",
        "1.   Review score > 3 = Positive Review [1]\n",
        "2.   Review score <= 3 = Negative Review [0]\n",
        "\n",
        "**Split Data:**\n",
        "\n",
        "We kept 80% of the data for training and 20% for testing.\n",
        "\n",
        "**Tokenization:**\n",
        "\n",
        "We tokenized all the text based on the words in X_train data and converted training and testing text data into a sequence. We saved the tokenizer to be used in inference pipeline.\n",
        "\n",
        "**Padding:**\n",
        "\n",
        "At the end, to keep all the data with same size, we padded the tokenized data with the maximum length of the text from the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "qivxAYGE9xTQ"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "  \"\"\" Class to pre-process the data \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "\n",
        "  def data_cleaning(self, text):\n",
        "    \"\"\" Clean the text data by removing punctuation, converting text to lower case, removing stopwords and applying stemming \"\"\"\n",
        "    text = text.str.replace('[^\\w\\s]', '')\n",
        "    text = text.str.lower()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "    stemmer = PorterStemmer()\n",
        "    text = text.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
        "    text = [re.sub(r'\\W', ' ', word) for word in text]\n",
        "    return text\n",
        "\n",
        "  def label_encodings(self, score):\n",
        "    \"\"\" Convert the score [1-5] into binary data [1-positive, 0-negative] \"\"\"\n",
        "    return int(score > 3)\n",
        "\n",
        "  def split_data(self, X,y,split_size):\n",
        "    \"\"\" Split the data into training and testing data \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "  def tokenization(self, X_train, X_test, vocab_size):\n",
        "    \"\"\" Tokenize the text data and convert the text into the sequence \"\"\"\n",
        "    tokenizer = Tokenizer(vocab_size)  # Adjust the vocabulary size as needed\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    return X_train_seq, X_test_seq, tokenizer\n",
        "\n",
        "  def save_tokenizer(self, tokenizer, tokenizer_file_path):\n",
        "    \"\"\" Save the tokenizer file to use for prediction in inference \"\"\"\n",
        "    with open(tokenizer_file_path, 'wb') as tokenizer_file:\n",
        "      pickle.dump(tokenizer, tokenizer_file)\n",
        "\n",
        "  def padding_sequence(self, X_train_seq, X_test_seq, max_len):\n",
        "    \"\"\" Pad the sequence to the maximum length of the text data \"\"\"\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "    return X_train_pad, X_test_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FFZ1IjqEM-a"
      },
      "source": [
        "# Model Training Pipeline\n",
        "\n",
        "**Approach-1: Training model from scratch**\n",
        "\n",
        "We used Bi-Directional LSTM architecture to train the model. Considering the size of the dataset, remembering the positive or negative words in the reviews from starting to ending is important. This was the main reason to select the LSTM architecture.\n",
        "\n",
        "'Adam' optimizer with 'binary_crossentropy' loss gave us the best results. We used 'sigmoid' as an activation function considering the binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "2CUrFMUEESMZ"
      },
      "outputs": [],
      "source": [
        "class ModelBuilder:\n",
        "  \"\"\" Class to build the model and train the model from start \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def build_model(self, max_len):\n",
        "    \"\"\"\n",
        "    Build a bidirectional LSTM model.\n",
        "    Using 'sigmoid' activation and 'binary_crossentropy' loss for binary classification\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=10000, output_dim=16, input_length=max_len))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def train_model(self, model, X_train, y_train, X_test, y_test, num_epochs):\n",
        "    \"\"\" Train the model with training data and validate with the testing data. Save the model. \"\"\"\n",
        "    model.fit(X_train, y_train, epochs= num_epochs, validation_data=(X_test_pad, y_test))\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    \"\"\" Print the model summary \"\"\"\n",
        "    print(\"\\n\")\n",
        "    print(model.summary())\n",
        "    model.save('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment.keras')\n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_m3HfkKUINI"
      },
      "source": [
        "# Fine-Tune Pre-Trained Model\n",
        "\n",
        "**Approach-2: Using pre-trained model and fine-tune with customized data**\n",
        "\n",
        "After training the sentiment analysis model we built from scratch, we decided to use a pre-trained transformer model called distil-BERT. We tried to fine-tune it with our food review dataset, where we freeze all the layers, except the last one, which is made trainable. Post training, we save the model and tokenizer to use it on command with new unseen food review data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "S7xBDS93dkru"
      },
      "outputs": [],
      "source": [
        "class FineTuningModel:\n",
        "  \"\"\" Class to use pre-trained DistilBERT model and fine-tune the model \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fine_tune_model_build(self, X_train, y_train, X_test, y_test):\n",
        "    \"\"\" Fine tune the BERT model and fine tune the model \"\"\"\n",
        "    X_train_tensor = tf.constant(X_train)\n",
        "    y_train_tensor = tf.constant(y_train)\n",
        "    X_test_tensor = tf.constant(X_test)\n",
        "    y_test_tensor = tf.constant(y_test)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
        "    eval_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor,y_test_tensor))\n",
        "\n",
        "    \"\"\" Build the tokenizer and get the tokens for training and testing data \"\"\"\n",
        "    tokenizer_ft = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    X_train_tokens = tokenizer_ft(X_train, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
        "    X_test_tokens = tokenizer_ft(X_test, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
        "\n",
        "    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "    \"\"\" Freeze all layers except the last layer \"\"\"\n",
        "    for layer in distilbert_model.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    \"\"\" Extract the logits from the DistilBERT model output \"\"\"\n",
        "    logits = distilbert_model(X_train_tokens)['logits']\n",
        "\n",
        "    \"\"\" Adding the customized trainable layers on the top of pre-trained model \"\"\"\n",
        "    model_ft = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    optimizer_ft = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_ft = tf.keras.losses.BinaryCrossentropy()\n",
        "    metrics_ft = ['accuracy']\n",
        "\n",
        "    \"\"\" Compile the fine-tuned model \"\"\"\n",
        "    model_ft.compile(optimizer=optimizer_ft, loss=loss_ft, metrics=metrics_ft)\n",
        "\n",
        "    \"\"\" Fit the model \"\"\"\n",
        "    model_ft.fit(\n",
        "        logits,\n",
        "        y_train,\n",
        "        validation_data=(distilbert_model(X_test_tokens)['logits'], y_test),\n",
        "        epochs=50,\n",
        "        batch_size=8\n",
        "    )\n",
        "\n",
        "    \"\"\" Print the model summary \"\"\"\n",
        "    print(\"\\n\")\n",
        "    print(model_ft.summary())\n",
        "\n",
        "    \"\"\" Save the model \"\"\"\n",
        "    model_ft.save('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment_finetuned.h5')\n",
        "\n",
        "    \"\"\" Evaluate the model \"\"\"\n",
        "    loss_ft, accuracy_ft = model_ft.evaluate(distilbert_model(X_test_tokens)['logits'], y_test, batch_size=32)\n",
        "    return tokenizer_ft, model_ft, loss_ft, accuracy_ft, distilbert_model\n",
        "\n",
        "  def save_fine_tuned_tokenizer(self, tokenizer_ft, tokenizer_ft_file_path):\n",
        "    \"\"\" Save the tokenizer from fine tuning of the model \"\"\"\n",
        "    with open(tokenizer_ft_file_path, 'wb') as tokenizer_file:\n",
        "      pickle.dump(tokenizer_ft, tokenizer_file)\n",
        "\n",
        "  def save_fine_tuned_model(self, model_ft, model_ft_file_path):\n",
        "    \"\"\" Save the fine-tuned model \"\"\"\n",
        "    model_ft.save(model_ft_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_kzNNJMGkEW"
      },
      "source": [
        "# Inference Pipeline\n",
        "\n",
        "After training both the models, now it was time to test the results and for that, we decided to create a class that loads the saved tokenizers, does the required pre-processing step for both the models respectively, and gives the prediction results for the unseen food review data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "LnWqvlQ6Zztm"
      },
      "outputs": [],
      "source": [
        "class SentimentPredictor:\n",
        "  \"\"\" Class to predict the sentiment with the trained model \"\"\"\n",
        "  def __init__(self):\n",
        "    self.data_preprocessor = DataProcessor()\n",
        "    self.model_builder = ModelBuilder()\n",
        "\n",
        "  def load_tokenizer(self, tokenizer_file_path):\n",
        "    \"\"\" Load saved tokenizer from the path \"\"\"\n",
        "    with open(tokenizer_file_path, 'rb') as tokenizer_file:\n",
        "      loaded_tokenizer = pickle.load(tokenizer_file)\n",
        "    return loaded_tokenizer\n",
        "\n",
        "  def load_fine_tuned_tokenizer(self, tokenizer_ft_file_path):\n",
        "    \"\"\" Load saved tokenizer for fine tuned model from the path \"\"\"\n",
        "    with open(tokenizer_ft_file_path, 'rb') as tokenizer_file:\n",
        "      loaded_tokenizer_ft = pickle.load(tokenizer_file)\n",
        "    return loaded_tokenizer_ft\n",
        "\n",
        "  def preprocess_input(self, text, tokenizer, max_len):\n",
        "    \"\"\" Pre-process the input, convert the text to sequence and add the padding \"\"\"\n",
        "    text_sequence = tokenizer.texts_to_sequences(text)\n",
        "    vocab_size = len(tokenizer.word_index)\n",
        "    text_padded = pad_sequences(text_sequence, maxlen = max_len, padding= 'post')\n",
        "    return text_padded\n",
        "\n",
        "  def load_trained_model(self, model_file_path):\n",
        "    \"\"\" Load the saved model \"\"\"\n",
        "    model = load_model(model_file_path)\n",
        "    return model\n",
        "\n",
        "  def predict_sentiment(self, text, model):\n",
        "    \"\"\" Predict and return the sentiment as 1 or 0 \"\"\"\n",
        "    prediction = model.predict(text)\n",
        "    if prediction > 0.5:\n",
        "      prediction = 1\n",
        "    else:\n",
        "      prediction = 0\n",
        "    return prediction\n",
        "\n",
        "  def preprocess_input_fine_tuned(self, text_ft, tokenizer_ft, max_len):\n",
        "    \"\"\" Pre-process the input text to feed to the fine tuned model \"\"\"\n",
        "    text_tokens = tokenizer_ft(text_ft, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
        "    return text_tokens\n",
        "\n",
        "  def predict_sentiment_fine_tuned(self, text_ft, model_ft, distilbert_model):\n",
        "    \"\"\" Get the prediction from the fine-tuned model \"\"\"\n",
        "    prediction = model_ft.predict(distilbert_model(text_ft)['logits'])[0, 0]\n",
        "    if prediction > 0.5:\n",
        "      prediction = 1\n",
        "    else:\n",
        "      prediction = 0\n",
        "    return prediction\n",
        "\n",
        "  def print_output(self, prediction):\n",
        "    \"\"\" Print the sentiment output for the user \"\"\"\n",
        "    if prediction == 1:\n",
        "      print(\"Positive Review\")\n",
        "    else:\n",
        "      print(\"Negative Review\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGAix7kMxvdI"
      },
      "source": [
        "# Main Function\n",
        "\n",
        "This function calls all the functions in the classes using objects.\n",
        "Here are the overall steps that the code is following:\n",
        "\n",
        "\n",
        "1.   Read the data.\n",
        "2.   Instantiate the object for DataProcessor.\n",
        "3.   Use the data processing object to call the functions from the class to pre-process the data\n",
        "4.   Instantiate the object for ModelBuilder (Approach-1).\n",
        "5.   Use the model training object to call the functions to train the model from scratch.\n",
        "6.   Instantiate the object for FineTuningModel (Approach-2).\n",
        "7.   Use the fine tuning model training object to call the function to process the data to be used with pre-trained model and then fine-tune the model.\n",
        "8.   Instantiate the object for SentimentPredictor (Inference pipeline).\n",
        "9.   Use the predictor object to call the functions to process the data to be give to the models, predict the output and print the output for both models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJfcqiIP6BUB",
        "outputId": "6a8ab9a5-1e30-478f-93fa-8da9c4339797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-104-6682ecd8ceeb>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  text = text.str.replace('[^\\w\\s]', '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model Training:\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 121s 469ms/step - loss: 0.4793 - accuracy: 0.7832 - val_loss: 0.3905 - val_accuracy: 0.8285\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 123s 493ms/step - loss: 0.3663 - accuracy: 0.8365 - val_loss: 0.4021 - val_accuracy: 0.8345\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 115s 459ms/step - loss: 0.3191 - accuracy: 0.8659 - val_loss: 0.3782 - val_accuracy: 0.8445\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 117s 468ms/step - loss: 0.2966 - accuracy: 0.8794 - val_loss: 0.3877 - val_accuracy: 0.8380\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 115s 459ms/step - loss: 0.2796 - accuracy: 0.8896 - val_loss: 0.3955 - val_accuracy: 0.8335\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 115s 461ms/step - loss: 0.2709 - accuracy: 0.8905 - val_loss: 0.3997 - val_accuracy: 0.8375\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 115s 461ms/step - loss: 0.2631 - accuracy: 0.8949 - val_loss: 0.3954 - val_accuracy: 0.8330\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 117s 467ms/step - loss: 0.2595 - accuracy: 0.8976 - val_loss: 0.3938 - val_accuracy: 0.8310\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 112s 449ms/step - loss: 0.2480 - accuracy: 0.9039 - val_loss: 0.4075 - val_accuracy: 0.8205\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 115s 460ms/step - loss: 0.2317 - accuracy: 0.9082 - val_loss: 0.4063 - val_accuracy: 0.8315\n",
            "63/63 [==============================] - 7s 118ms/step - loss: 0.4063 - accuracy: 0.8315\n",
            "\n",
            "\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_17 (Embedding)    (None, 469, 16)           160000    \n",
            "                                                                 \n",
            " bidirectional_17 (Bidirect  (None, 128)               41472     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201601 (787.50 KB)\n",
            "Trainable params: 201601 (787.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Fine-Tuned Model Training:\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 2s 36ms/step - loss: 0.6288 - accuracy: 0.7900 - val_loss: 0.5508 - val_accuracy: 0.8100\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.4615 - accuracy: 0.8500 - val_loss: 0.5090 - val_accuracy: 0.8100\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.4574 - accuracy: 0.8500 - val_loss: 0.4933 - val_accuracy: 0.8100\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 0.4365 - accuracy: 0.8500 - val_loss: 0.4840 - val_accuracy: 0.8100\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.4383 - accuracy: 0.8500 - val_loss: 0.4848 - val_accuracy: 0.8100\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4280 - accuracy: 0.8500 - val_loss: 0.4902 - val_accuracy: 0.8100\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 0.4353 - accuracy: 0.8500 - val_loss: 0.5028 - val_accuracy: 0.8100\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.4320 - accuracy: 0.8500 - val_loss: 0.4860 - val_accuracy: 0.8100\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 0.4251 - accuracy: 0.8500 - val_loss: 0.4868 - val_accuracy: 0.8100\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4322 - accuracy: 0.8500 - val_loss: 0.4939 - val_accuracy: 0.8100\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4290 - accuracy: 0.8500 - val_loss: 0.4910 - val_accuracy: 0.8100\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4327 - accuracy: 0.8500 - val_loss: 0.4878 - val_accuracy: 0.8100\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4235 - accuracy: 0.8500 - val_loss: 0.4927 - val_accuracy: 0.8100\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4290 - accuracy: 0.8500 - val_loss: 0.5033 - val_accuracy: 0.8100\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4318 - accuracy: 0.8500 - val_loss: 0.4884 - val_accuracy: 0.8100\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4283 - accuracy: 0.8500 - val_loss: 0.4880 - val_accuracy: 0.8100\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.4283 - accuracy: 0.8500 - val_loss: 0.5022 - val_accuracy: 0.8100\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4157 - accuracy: 0.8500 - val_loss: 0.4891 - val_accuracy: 0.8100\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.4254 - accuracy: 0.8500 - val_loss: 0.4906 - val_accuracy: 0.8100\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4211 - accuracy: 0.8500 - val_loss: 0.4965 - val_accuracy: 0.8100\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4501 - accuracy: 0.8500 - val_loss: 0.4969 - val_accuracy: 0.8100\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4351 - accuracy: 0.8500 - val_loss: 0.4898 - val_accuracy: 0.8100\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4195 - accuracy: 0.8500 - val_loss: 0.4965 - val_accuracy: 0.8100\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4252 - accuracy: 0.8500 - val_loss: 0.5161 - val_accuracy: 0.8100\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4228 - accuracy: 0.8500 - val_loss: 0.4911 - val_accuracy: 0.8100\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4333 - accuracy: 0.8500 - val_loss: 0.4920 - val_accuracy: 0.8100\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.4403 - accuracy: 0.8500 - val_loss: 0.5208 - val_accuracy: 0.8100\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8500 - val_loss: 0.4912 - val_accuracy: 0.8100\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4312 - accuracy: 0.8500 - val_loss: 0.5010 - val_accuracy: 0.8100\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4208 - accuracy: 0.8500 - val_loss: 0.4929 - val_accuracy: 0.8100\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4247 - accuracy: 0.8500 - val_loss: 0.5024 - val_accuracy: 0.8100\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4227 - accuracy: 0.8500 - val_loss: 0.4952 - val_accuracy: 0.8100\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4228 - accuracy: 0.8500 - val_loss: 0.4980 - val_accuracy: 0.8100\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4215 - accuracy: 0.8500 - val_loss: 0.5099 - val_accuracy: 0.8100\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4055 - accuracy: 0.8500 - val_loss: 0.4988 - val_accuracy: 0.8100\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4243 - accuracy: 0.8500 - val_loss: 0.5007 - val_accuracy: 0.8100\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4168 - accuracy: 0.8500 - val_loss: 0.5099 - val_accuracy: 0.8100\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4178 - accuracy: 0.8500 - val_loss: 0.5008 - val_accuracy: 0.8100\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4117 - accuracy: 0.8500 - val_loss: 0.5138 - val_accuracy: 0.8100\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4148 - accuracy: 0.8500 - val_loss: 0.5115 - val_accuracy: 0.8100\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4239 - accuracy: 0.8500 - val_loss: 0.5069 - val_accuracy: 0.8100\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4327 - accuracy: 0.8500 - val_loss: 0.5389 - val_accuracy: 0.8100\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4299 - accuracy: 0.8500 - val_loss: 0.5010 - val_accuracy: 0.8100\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4113 - accuracy: 0.8500 - val_loss: 0.5213 - val_accuracy: 0.8100\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.4042 - accuracy: 0.8500 - val_loss: 0.5141 - val_accuracy: 0.8100\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4125 - accuracy: 0.8500 - val_loss: 0.5078 - val_accuracy: 0.8100\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.4037 - accuracy: 0.8500 - val_loss: 0.5238 - val_accuracy: 0.8100\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4077 - accuracy: 0.8500 - val_loss: 0.5216 - val_accuracy: 0.8100\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4043 - accuracy: 0.8500 - val_loss: 0.5179 - val_accuracy: 0.8100\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.4051 - accuracy: 0.8500 - val_loss: 0.5271 - val_accuracy: 0.8100\n",
            "\n",
            "\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_15 (Flatten)        (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 256)               768       \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_335 (Dropout)       (None, 256)               0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132609 (518.00 KB)\n",
            "Trainable params: 132609 (518.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5271 - accuracy: 0.8100\n",
            "\n",
            "\n",
            "\n",
            "Sample Text For Model:  Perfect size sea salt for the table or the picnic basket.  We love it. Shakes well, no clumping and flows freely.\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Positive Review\n",
            "\n",
            "\n",
            "\n",
            "Sample Text For Fine-Tuned Model:  Perfect size sea salt for the table or the picnic basket.  We love it. Shakes well, no clumping and flows freely.\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "Positive Review\n",
            "\n",
            "\n",
            "\n",
            "Sample Predictions: \n",
            "\n",
            " Great food! I love the idea of one food for all ages & breeds. A real convenience as well as a really good product. \n",
            "\n",
            "1/1 [==============================] - 1s 895ms/step\n",
            "Model Prediction: \n",
            "Positive Review\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Fine-Tuned Model Prediction: \n",
            "Positive Review\n",
            "\n",
            " The worst products I ever tried in my life. Very bad quality and bad service. \n",
            "\n",
            "1/1 [==============================] - 1s 879ms/step\n",
            "Model Prediction: \n",
            "Negative Review\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Fine-Tuned Model Prediction: \n",
            "Negative Review\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \"\"\" Mount the drive \"\"\"\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  \"\"\" Read the data \"\"\"\n",
        "  reviews = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/Reviews.csv').head(10000)\n",
        "\n",
        "  \"\"\" Instantiate DataProcessor class to preprocess the data \"\"\"\n",
        "  data_processor = DataProcessor()\n",
        "\n",
        "  \"\"\" Clean the text in the 'Text' column of DataFrame \"\"\"\n",
        "  texts = data_processor.data_cleaning(reviews['Text'])\n",
        "\n",
        "  \"\"\" Encode the scores in binary labels \"\"\"\n",
        "  labels = reviews['Score'].apply(data_processor.label_encodings)\n",
        "\n",
        "  \"\"\" Split the data into training and testing data \"\"\"\n",
        "  X_train, X_test, y_train, y_test = data_processor.split_data(texts, labels, split_size=0.2)\n",
        "\n",
        "  \"\"\" Sent the vocabulary size for tokenization \"\"\"\n",
        "  vocab_size = 1000\n",
        "\n",
        "  \"\"\" Tokenize the training and testing data \"\"\"\n",
        "  X_train_seq, X_test_seq, tokenizer = data_processor.tokenization(X_train, X_test, vocab_size)\n",
        "\n",
        "  \"\"\" Finding maximum sequence length \"\"\"\n",
        "  max_len = max(len(seq) for seq in X_train_seq)\n",
        "\n",
        "  \"\"\" Pad the training and testing data sequence with maximum length \"\"\"\n",
        "  X_train_pad, X_test_pad = data_processor.padding_sequence(X_train_seq, X_test_seq, max_len)\n",
        "\n",
        "  \"\"\" Save the tokenizer \"\"\"\n",
        "  data_processor.save_tokenizer(tokenizer, '/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/tokenizer.pkl')\n",
        "\n",
        "  \"\"\" Instantiate ModelBuilder class to build and train the model \"\"\"\n",
        "  model_builder = ModelBuilder()\n",
        "\n",
        "  \"\"\" Build the model with maximum sequence length \"\"\"\n",
        "  print(\"\\n\\nModel Training:\\n\\n\")\n",
        "  model = model_builder.build_model(max_len)\n",
        "\n",
        "  \"\"\" Train the model and obtain loss and accuracy \"\"\"\n",
        "  loss, accuracy = model_builder.train_model(model, X_train_pad, y_train, X_test_pad, y_test, num_epochs=10)\n",
        "\n",
        "  \"\"\" Instantiate FineTuningModel class for fine-tuning the model \"\"\"\n",
        "  fine_tuning_model = FineTuningModel()\n",
        "\n",
        "  \"\"\" Build and fine tune the model. Reducing size to train the model with limited computational power. \"\"\"\n",
        "  print(\"\\n\\nFine-Tuned Model Training:\\n\\n\")\n",
        "  X_train = X_train[0:100]\n",
        "  y_train = y_train[0:100]\n",
        "  X_test = X_test[0:100]\n",
        "  y_test = y_test[0:100]\n",
        "  tokenizer_ft, model_ft, loss_ft, accuracy_ft, distilbert_model = fine_tuning_model.fine_tune_model_build(X_train, y_train, X_test, y_test)\n",
        "\n",
        "  \"\"\" Save the fine-tuned tokenizer \"\"\"\n",
        "  fine_tuning_model.save_fine_tuned_tokenizer(tokenizer_ft, '/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/tokenizer_ft.pkl')\n",
        "\n",
        "  \"\"\" Save the fine-tuned model \"\"\"\n",
        "  fine_tuning_model.save_fine_tuned_model(model_ft, '/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment_finetuned.keras')\n",
        "\n",
        "\n",
        "  \"\"\" PREDICT THE SENTIMENT \"\"\"\n",
        "\n",
        "  \"\"\" Instantiate SentimentPredictor class to predict sentiment \"\"\"\n",
        "  sentiment_predictor = SentimentPredictor()\n",
        "\n",
        "  \"\"\" Load the tokenizer \"\"\"\n",
        "  tokenizer = sentiment_predictor.load_tokenizer('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/tokenizer.pkl')\n",
        "\n",
        "  \"\"\" Take input from the user \"\"\"\n",
        "  print(\"\\n\\n\")\n",
        "  text = \"Perfect size sea salt for the table or the picnic basket.  We love it. Shakes well, no clumping and flows freely.\"\n",
        "  print(\"Sample Text For Model: \", text)\n",
        "\n",
        "  \"\"\" Pad the input text  \"\"\"\n",
        "  text_padded = sentiment_predictor.preprocess_input([text], tokenizer, max_len)\n",
        "\n",
        "  \"\"\" Train the model \"\"\"\n",
        "  model = sentiment_predictor.load_trained_model('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment.keras')\n",
        "\n",
        "  \"\"\" Predict the sentiment \"\"\"\n",
        "  prediction = sentiment_predictor.predict_sentiment(text_padded, model)\n",
        "\n",
        "  \"\"\" Print the prediction \"\"\"\n",
        "  sentiment_predictor.print_output(prediction)\n",
        "\n",
        "  \"\"\" Instantiate another SentimentPredictor class for fine-tuned model inference \"\"\"\n",
        "  fine_tuned_predictor = SentimentPredictor()\n",
        "\n",
        "  \"\"\" Load the fine-tuned model tokenizer for inference pipeline \"\"\"\n",
        "  tokenizer_ft = fine_tuned_predictor.load_fine_tuned_tokenizer('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/tokenizer_ft.pkl')\n",
        "\n",
        "  \"\"\" Get the input from the user to predict with fine-tuned model \"\"\"\n",
        "  print(\"\\n\\n\")\n",
        "  text_ft = \"Perfect size sea salt for the table or the picnic basket.  We love it. Shakes well, no clumping and flows freely.\"\n",
        "  print(\"Sample Text For Fine-Tuned Model: \", text_ft)\n",
        "\n",
        "  \"\"\" Pre-process the input for predicting with fine-tuned model \"\"\"\n",
        "  text_tokens = fine_tuned_predictor.preprocess_input_fine_tuned([text_ft], tokenizer_ft, max_len)\n",
        "\n",
        "  \"\"\" Load the fine-tuned model for prediction \"\"\"\n",
        "  model_ft = fine_tuned_predictor.load_trained_model('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment_finetuned.keras')\n",
        "\n",
        "  \"\"\" Predict the sentiment using the fine-tuned model \"\"\"\n",
        "  prediction_ft = fine_tuned_predictor.predict_sentiment_fine_tuned(text_tokens, model_ft, distilbert_model)\n",
        "\n",
        "  \"\"\" Print the output \"\"\"\n",
        "  fine_tuned_predictor.print_output(prediction_ft)\n",
        "\n",
        "  \"\"\" MAKING MULTIPLE PREDICTIONS \"\"\"\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"Sample Predictions: \")\n",
        "  test_texts = [\n",
        "    \"Great food! I love the idea of one food for all ages & breeds. A real convenience as well as a really good product.\",\n",
        "    \"The worst products I ever tried in my life. Very bad quality and bad service.\"\n",
        "  ]\n",
        "  for text in test_texts:\n",
        "    print(\"\\n\",text,\"\\n\")\n",
        "    text_padded = sentiment_predictor.preprocess_input([text], tokenizer, max_len)\n",
        "    model = sentiment_predictor.load_trained_model('/content/drive/MyDrive/Colab Notebooks/projects/food_sentiment/food_sentiment.keras')\n",
        "    prediction = sentiment_predictor.predict_sentiment(text_padded, model)\n",
        "    print(\"Model Prediction: \")\n",
        "    sentiment_predictor.print_output(prediction)\n",
        "\n",
        "    text_tokens = fine_tuned_predictor.preprocess_input_fine_tuned([text], tokenizer_ft, max_len)\n",
        "    prediction_ft = fine_tuned_predictor.predict_sentiment_fine_tuned(text_tokens, model_ft, distilbert_model)\n",
        "    print(\"Fine-Tuned Model Prediction: \")\n",
        "    fine_tuned_predictor.print_output(prediction_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grgmLDJfxTMq"
      },
      "source": [
        "**Completion Note:**\n",
        "\n",
        "Due to limited computation power and RAM, the training of fine-tuned data is performed with limited data. Hence, the accuracy of prediction with fine-tuned model is less than the model trained from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "7UDFvgZDxoEd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
